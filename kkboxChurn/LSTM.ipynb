{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import csv\n",
    "from random import shuffle\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import metrics\n",
    "from math import sqrt\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "gpu_options = tf.GPUOptions(allow_growth = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# flags\n",
    "tf.flags.DEFINE_float(\"epsilon\", 0.1, \"Epsilon value for Adam Optimizer.\")\n",
    "tf.flags.DEFINE_float(\"learning_rate\", 0.1, \"Learning rate\")\n",
    "tf.flags.DEFINE_float(\"max_grad_norm\", 20.0, \"Clip gradients to this norm.\")\n",
    "tf.flags.DEFINE_float(\"keep_prob\", 0.3, \"Keep probability for dropout\")\n",
    "tf.flags.DEFINE_integer(\"hidden_layer_num\", 1, \"The number of hidden layers (Integer)\")\n",
    "tf.flags.DEFINE_integer(\"hidden_size\", 200, \"The number of hidden nodes (Integer)\")\n",
    "tf.flags.DEFINE_integer(\"preprocess_size\", 200, \"The number of preprocess nodes after one-hot (Integer)\")\n",
    "tf.flags.DEFINE_integer(\"embedding_size\", 300, \"The number of nodes for word embedding (Integer)\")\n",
    "tf.flags.DEFINE_integer(\"evaluation_interval\", 5, \"Evaluate and print results every x epochs\")\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 32, \"Batch size for training.\")\n",
    "tf.flags.DEFINE_integer(\"epochs\", 100, \"Number of epochs to train for.\")\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "tf.flags.DEFINE_string(\"train_data_path\", 'train_v2.npy', \"Path to the training dataset\")\n",
    "# tf.flags.DEFINE_string(\"test_data_path\", 'data/2012_assist_test.csv', \"Path to the testing dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# output_path = 'output.npy'\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "# log_file_path = FLAGS.train_data_path[5:-4] + 'l2'  + '.txt'\n",
    "log_file_path = 'log.txt'\n",
    "# hidden_state_path =FLAGS.train_data_path[5:-4] + str(FLAGS.hidden_layer_num) + '.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=32\n",
      "EMBEDDING_SIZE=300\n",
      "EPOCHS=100\n",
      "EPSILON=0.1\n",
      "EVALUATION_INTERVAL=5\n",
      "HIDDEN_LAYER_NUM=1\n",
      "HIDDEN_SIZE=200\n",
      "KEEP_PROB=0.3\n",
      "LEARNING_RATE=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "MAX_GRAD_NORM=20.0\n",
      "PREPROCESS_SIZE=200\n",
      "TRAIN_DATA_PATH=train_v2.npy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HyperParamsConfig(object):\n",
    "    \"\"\"Small config.\"\"\"\n",
    "    init_scale = 0.05\n",
    "    num_steps = 0\n",
    "    max_grad_norm = FLAGS.max_grad_norm\n",
    "    max_max_epoch = FLAGS.epochs\n",
    "    keep_prob = FLAGS.keep_prob\n",
    "    num_skills = 0\n",
    "    state_size = [200]\n",
    "    beta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KKBoxModel(object):\n",
    "\n",
    "    def __init__(self, is_training, config):\n",
    "        self.state_size = config.state_size\n",
    "        self._batch_size = batch_size = FLAGS.batch_size\n",
    "        self.input_dimensions = input_dimensions = config.input_dimensions\n",
    "        self.hidden_layer_num = len(self.state_size)\n",
    "        self.hidden_size = size = FLAGS.hidden_size\n",
    "        self.num_steps = num_steps = config.num_steps\n",
    "        input_size = input_dimensions\n",
    "#         preprocess_size = FLAGS.preprocess_size\n",
    "#         embedding_size = FLAGS.embedding_size\n",
    "        inputs = self._input_data = tf.placeholder(tf.float32, [batch_size, num_steps])\n",
    "#         self._target_id = target_id = tf.placeholder(tf.int32, [None])\n",
    "#         input_vectors = self._input_vector = tf.placeholder(tf.float32, [batch_size,num_steps , embedding_size ])\n",
    "        self._target_correctness = target_correctness = tf.placeholder(tf.float32, [None])\n",
    "        final_hidden_size = self.state_size[-1]\n",
    "\n",
    "        hidden_layers = []\n",
    "        # input_vectors = tf.reshape(input_vectors, [batch_size,num_steps ,preprocess_size ])\n",
    "        for i in range(self.hidden_layer_num):\n",
    "            \n",
    "            hidden1 = tf.contrib.rnn.BasicLSTMCell(self.state_size[i], state_is_tuple=True,reuse=tf.get_variable_scope().reuse)\n",
    "            if is_training and config.keep_prob < 1:\n",
    "                hidden1 = tf.contrib.rnn.DropoutWrapper(hidden1, output_keep_prob=FLAGS.keep_prob)\n",
    "            hidden_layers.append(hidden1)\n",
    "        \n",
    "        cell = tf.contrib.rnn.MultiRNNCell(hidden_layers, state_is_tuple=True)\n",
    "\n",
    "        #input_data: [batch_size*num_steps]\n",
    "        input_data = tf.reshape(self._input_data, [-1])\n",
    "        \n",
    "        inputs = tf.reshape(input_data,[-1,num_steps,input_size])\n",
    "#         print input_data.shape\n",
    "        #one-hot encoding\n",
    "#         with tf.device(\"/gpu:0\"):\n",
    "#             #labels: [batch_size* num_steps, 1]\n",
    "#             labels = tf.expand_dims(input_data, 1)\n",
    "#             #indices: [batch_size*num_steps, 1]\n",
    "#             indices = tf.expand_dims(tf.range(0, batch_size*num_steps, 1), 1)\n",
    "#             #concated: [batch_size * num_steps, 2]\n",
    "#             concated = tf.concat( [indices, labels],1)\n",
    "\n",
    "#             # If sparse_indices is an n by d matrix, then for each i in [0, n)\n",
    "#             # dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]\n",
    "#             # input_size: 2* num_skills\n",
    "#             # inputs: [batch_size* num_steps * input_size]\n",
    "#             inputs = tf.sparse_to_dense(concated, tf.stack([batch_size*num_steps, input_size]), 1.0, 0.0)\n",
    "#             inputs.set_shape([batch_size*num_steps, input_size])\n",
    "\n",
    "        # [batch_size, num_steps, input_size]\n",
    "        \n",
    "        inputs = tf.reshape(inputs, [-1, num_steps, input_size])\n",
    "        x = inputs\n",
    "        # x = tf.transpose(inputs, [1, 0, 2])\n",
    "        # # Reshape to (n_steps*batch_size, n_input)\n",
    "        # x = tf.reshape(x, [-1, input_size])\n",
    "        # # Split to get a list of 'n_steps'\n",
    "        # # tensors of shape (doc_num, n_input)\n",
    "        # x = tf.split(0, num_steps, x)\n",
    "        #inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, num_steps, inputs)]\n",
    "        #outputs, state = tf.nn.rnn(hidden1, x, dtype=tf.float32)\n",
    "        \n",
    "        \n",
    "        # 'outputs' is a tensor of shape [batch_size, max_time, cell_state_size]\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x, dtype=tf.float32)\n",
    "        output = tf.reshape(tf.concat(outputs,1), [-1, final_hidden_size])\n",
    "        # calculate the logits from last hidden layer to output layer\n",
    "        \n",
    "        \n",
    "        sigmoid_w = tf.get_variable(\"sigmoid_w\", [final_hidden_size, 1])\n",
    "        sigmoid_b = tf.get_variable(\"sigmoid_b\", [1])\n",
    "        logits = tf.matmul(output, sigmoid_w) + sigmoid_b\n",
    " \n",
    "         # from output nodes to pick up the right one we want\n",
    "        logits = tf.reshape(logits, [-1])\n",
    "#         self._last_logits = logits[(batch_size)*(num_steps-1):,:]\n",
    "        selected_logits = logits\n",
    " \n",
    "         #make prediction\n",
    "        self._pred = self._pred_values = pred_values = tf.sigmoid(selected_logits)\n",
    " \n",
    "         # loss function\n",
    "        loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits = selected_logits,labels= target_correctness))\n",
    " \n",
    "        # loss = tf.reduce_sum(loss + config.beta * tf.norm(lstm_weights))\n",
    "        # loss = tf.reduce_sum(loss + config.beta * tf.nn.l2_loss(sigmoid_w))\n",
    "        # loss += \n",
    "\n",
    "        #self._cost = cost = tf.reduce_mean(loss)\n",
    "        self._final_state = state\n",
    "        self._cost = cost = loss\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self._batch_size\n",
    "\n",
    "\n",
    "    @property\n",
    "    def input_data(self):\n",
    "        return self._input_data\n",
    "\n",
    "    @property\n",
    "    def input_vector(self):\n",
    "        return self._input_vector\n",
    "\n",
    "    @property\n",
    "    def auc(self):\n",
    "        return self._auc\n",
    "\n",
    "    @property\n",
    "    def pred(self):\n",
    "        return self._pred\n",
    "\n",
    "    @property\n",
    "    def target_id(self):\n",
    "        return self._target_id\n",
    "\n",
    "    @property\n",
    "    def target_correctness(self):\n",
    "        return self._target_correctness\n",
    "\n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        return self._initial_state\n",
    "\n",
    "    @property\n",
    "    def pred_values(self):\n",
    "        return self._pred_values\n",
    "\n",
    "    @property\n",
    "    def cost(self):\n",
    "        return self._cost\n",
    "\n",
    "    @property\n",
    "    def final_state(self):\n",
    "        return self._final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(session, m, users, labels, eval_op, verbose=False):\n",
    "    \"\"\"Runs the model on the given data.\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    index = 0\n",
    "    pred_labels = []\n",
    "    actual_labels = []\n",
    "    print labels.shape\n",
    "    while(index+m.batch_size < len(users)):\n",
    "        x = np.zeros((m.batch_size, m.num_steps))\n",
    "\n",
    "        target_correctness = []\n",
    "#         problem_context = np.zeros((m.batch_size, m.num_steps, FLAGS.embedding_size))\n",
    "        count = 0\n",
    "        for i in range(m.batch_size):\n",
    "            user = users[index+i]\n",
    "#             problem_ids = student[1]\n",
    "#             skill_ids = student[2]\n",
    "            num_step = user.shape[0]\n",
    "            target_correctness = labels[index+i]\n",
    "#             print target_correctness.shape\n",
    "#             print user.shape\n",
    "            actual_labels += target_correctness\n",
    "#             for j in range(num_step):\n",
    "# #                 skill_id = int(skill_ids[j])\n",
    "# #                 problem_id = int(problem_ids[j])\n",
    "# #                 context = context_df.loc[context_df['problem_ids'] == problem_id, 'problem_vectors'].iloc[0]\n",
    "#                 # print context\n",
    "# #                 label_index = 0\n",
    "#                 if(int(correctness[j]) == 0):\n",
    "#                     label_index = skill_id\n",
    "#                 else:\n",
    "#                     label_index = skill_id + m.num_skills\n",
    "#                 x[i, j] = label_index\n",
    "                \n",
    "#                 problem_context[i,j] = context\n",
    "#                 target_id.append(i*m.num_steps*m.num_skills+j*m.num_skills+int(skill_ids[j+1]))\n",
    "#                 target_correctness.append(int(correctness[j+1]))\n",
    "#                 actual_labels.append(int(correctness[j+1]))\n",
    "                # problem_context.append(context)\n",
    "\n",
    "        pred, _, final_state, cost = session.run([m.pred, eval_op, m.final_state, m.cost], feed_dict={\n",
    "            m.input_data: user,\n",
    "            m.target_correctness: target_correctness})\n",
    "        \n",
    "        index += m.batch_size\n",
    "        \n",
    "\n",
    "\n",
    "        for p in pred:\n",
    "            pred_labels.append(p)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    rmse = sqrt(mean_squared_error(actual_labels, pred_labels))\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(actual_labels, pred_labels, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #calculate r^2\n",
    "    r2 = r2_score(actual_labels, pred_labels)\n",
    "    return rmse, auc, r2, final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_gradient_noise(t, stddev=1e-3, name=None):\n",
    "    \"\"\"\n",
    "    Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
    "    The input Tensor `t` should be a gradient.\n",
    "    The output will be `t` + gaussian noise.\n",
    "    0.001 was said to be a good fixed value for memory networks [2].\n",
    "    tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
    "    \"\"\"\n",
    "    with tf.name_scope( name, \"add_gradient_noise\",[t, stddev]) as name:\n",
    "        t = tf.convert_to_tensor(t, name=\"t\")\n",
    "        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n",
    "        return tf.add(t, gn, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_args):\n",
    "    \n",
    "    config = HyperParamsConfig()\n",
    "    eval_config = HyperParamsConfig()\n",
    "    timestamp = str(time.time())\n",
    "    train_data_path = FLAGS.train_data_path\n",
    "    #path to your test data set\n",
    "#     test_data_path = FLAGS.test_data_path\n",
    "    #the file to store your test results\n",
    "    result_file_path = \"run_logs_{}\".format(timestamp)\n",
    "    #your model name\n",
    "    model_name = \"Deep KKBOX\"\n",
    "\n",
    "    train_users = np.load('train_v2.npy')\n",
    "    train_label = np.load('active_users_label.npy')\n",
    "    train_max_steps = 0\n",
    "    for train_user in train_users:\n",
    "        if len(train_user) > train_max_steps:\n",
    "            train_max_steps = len(train_users)\n",
    "    \n",
    "    train_input_dimensions = train_users[0].shape[1]\n",
    "    config.num_steps = train_max_steps\n",
    "    \n",
    "    config.input_dimensions = train_input_dimensions\n",
    "#     test_students, test_max_num_problems, test_max_skill_num = read_data_from_csv_file(test_data_path)\n",
    "#     eval_config.num_steps = test_max_num_problems\n",
    "#     eval_config.num_skills = test_max_skill_num\n",
    "\n",
    "    with tf.Graph().as_default():\n",
    "        session_conf = tf.ConfigProto(allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "                                      log_device_placement=FLAGS.log_device_placement,\n",
    "                                      gpu_options=gpu_options)\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        # decay learning rate\n",
    "        starter_learning_rate = FLAGS.learning_rate\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step, 3000, 0.96, staircase=True)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=FLAGS.epsilon)\n",
    "\n",
    "        with tf.Session(config=session_conf) as session:\n",
    "\n",
    "            initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
    "\n",
    "            # training model\n",
    "            with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n",
    "                m = KKBoxModel(is_training=True, config=config)\n",
    "            # testing model\n",
    "#             with tf.variable_scope(\"model\", reuse=True, initializer=initializer):\n",
    "#                 mtest = StudentModel(is_training=False, config=eval_config)\n",
    "\n",
    "            grads_and_vars = optimizer.compute_gradients(m.cost)\n",
    "            grads_and_vars = [(tf.clip_by_norm(g, FLAGS.max_grad_norm), v)\n",
    "                              for g, v in grads_and_vars if g is not None]\n",
    "            grads_and_vars = [(add_gradient_noise(g), v) for g, v in grads_and_vars]\n",
    "            train_op = optimizer.apply_gradients(grads_and_vars, name=\"train_op\", global_step=global_step)\n",
    "            session.run(tf.global_variables_initializer())\n",
    "            # log hyperparameters to results file\n",
    "            # with open(result_file_path, \"a+\") as f:\n",
    "            #     print(\"Writing hyperparameters into file\")\n",
    "            #     f.write(\"Hidden layer size: %d \\n\" % (FLAGS.hidden_size))\n",
    "            #     f.write(\"Dropout rate: %.3f \\n\" % (FLAGS.keep_prob))\n",
    "            #     f.write(\"Batch size: %d \\n\" % (FLAGS.batch_size))\n",
    "            #     f.write(\"Max grad norm: %d \\n\" % (FLAGS.max_grad_norm))\n",
    "            # saver = tf.train.Saver(tf.all_variables())\n",
    "            \n",
    "            for i in range(config.max_max_epoch):\n",
    "                rmse, auc, r2, final_state = run_epoch(session, m, train_users, train_label, train_op, verbose=True)\n",
    "                print(\"Epoch: %d Train Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f \\n\" % (i + 1, rmse, auc, r2))\n",
    "                with open(log_file_path, \"a+\") as f:\n",
    "                    f.write(\"Epoch: %d Train Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f \\n\" % (i + 1, rmse, auc, r2))\n",
    "                if((i+1) % FLAGS.evaluation_interval == 0):\n",
    "                    print \"Save variables to disk\"\n",
    "                    # save_path = saver.save(session, model_name)#\n",
    "                    print(\"*\"*10)\n",
    "#                     print(\"Start to test model....\")\n",
    "#                     rmse, auc, r2, _ = run_epoch(session, mtest, test_students, tf.no_op())\n",
    "#                     print(\"Epoch: %d Test Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f\" % (i+1, rmse, auc, r2))\n",
    "#                     with open(log_file_path, \"a+\") as f:\n",
    "#                         f.write(\"Epoch: %d Test Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f\" % ((i+1) , rmse, auc, r2))\n",
    "#                         f.write(\"\\n\")\n",
    "\n",
    "#                         print(\"*\"*10)\n",
    "\n",
    "                # c, h = final_state\n",
    "                # if len(cs) < 1:\n",
    "                #     cs = c\n",
    "                # else \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'lables' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-ce29dcffb130>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/data/khuangaf/miniconda2/envs/py27/lib/python2.7/site-packages/tensorflow/python/platform/app.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     46\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-d87323b51c86>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_max_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_users\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d Train Metrics:\\n rmse: %.3f \\t auc: %.3f \\t r2: %.3f \\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a+\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-7adbeaf3b9af>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, m, users, labels, eval_op, verbose)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mactual_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mlables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'lables' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
